{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested with Python 3.12\n",
    "\n",
    "from _includes import Chat, ChatHistory, config, models, vars\n",
    "\n",
    "# File settings\n",
    "config.history_path=\"story.md\"\n",
    "config.separator='----'\n",
    "\n",
    "# Print conversation history in cell outputs\n",
    "config.print_messages=True \n",
    "\n",
    "# Reasoning settings\n",
    "    # Note: old reasoning will be removed automatically from the request - it does not have to be removed from the file\n",
    "    # Note: different inference providers output reasoning differently, which may result in <think> tags missing in the output\n",
    "    # Note: some providers do not allow to define reasoning. These providers can be excluded in openrouter web ui settings.\n",
    "\n",
    "config.write_reasoning = True # Write reasoning to md file\n",
    "config.print_reasoning = False # Print reasoning in cell outputs\n",
    "config.reasoning_header = '### Reasoning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.first_prompt = f'''\n",
    "**Story summary:**\n",
    "This is a story about a Red Hood\n",
    "\n",
    "**Characters:**\n",
    "Red Hood - a little girl\n",
    "Wolf - a big bad wolf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These letters in user prompt's text will be replaced with full names\n",
    "\n",
    "config.abbreviations.update({\n",
    "    'R': 'Red Hood', \n",
    "    'W': 'Wolf'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window with function buttons (optional)\n",
    "\n",
    "from _includes.ui import Chat_UI\n",
    "Chat_UI.start()\n",
    "config.print_messages = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create listener\n",
    "\n",
    "from _includes.listener import server_thread\n",
    "config.create_listener = True\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a scene - Deepseek R1\n",
    "Chat.write_scene(models['Deepseek R1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt - will not have a pre-prompt and post-prompt instructions\n",
    "config.user_prompt = '''\n",
    "Offer several options for story continuation in a form of story cards\n",
    "'''\n",
    "Chat.custom_prompt(models['Deepseek R1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a scene - Qwen 72b\n",
    "Chat.write_scene(models['Qwen 72b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine a part of the story according to guidelines in the user_prompt and prompt_vars\n",
    "# The story is devided into parts by config.separator\n",
    "# Part text and technical instructions are appended to user_prompt automatically - do not paste it manually\n",
    "\n",
    "config.user_prompt = f'''\n",
    "Change story tense from third person to first person\n",
    "For instance: \"Red Hood stood\" -> \"I stood\"\n",
    "'''\n",
    "\n",
    "config.print_output = True\n",
    "Chat.refine(models['Deepseek V3'], part=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite a part of the story according to guidelines in the user_prompt\n",
    "\n",
    "config.user_prompt = f'''\n",
    "Enrich the story part with dialogue\n",
    "'''\n",
    "\n",
    "config.print_output = True\n",
    "Chat.rewrite(models['Deepseek V3'], part=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate a part of the story\n",
    "\n",
    "config.user_prompt = f'''\n",
    "Mother gives Little Red Riding Hood a basket of food.\n",
    "'''\n",
    "\n",
    "config.print_output = True\n",
    "Chat.regenenerate(models['Deepseek V3'], part=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a part to the story after a specified part\n",
    "\n",
    "config.user_prompt = f'''\n",
    "Mother gives Little Red Riding Hood a basket of food.\n",
    "'''\n",
    "\n",
    "config.print_output = True\n",
    "Chat.add_part(models['Deepseek V3'], part=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the story\n",
    "# Summarized parts will replace the original parts in the prompt\n",
    "# This will help reduce the number of tokens in the prompt\n",
    "# Adding or deleting story parts in the middle of the story will require a new summary or a manual summary update\n",
    "\n",
    "Chat.summarize(models['Deepseek V3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize new story parts that were not yet summarized\n",
    "Chat.update_summary(models['Deepseek V3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last response from md file\n",
    "ChatHistory.remove_last_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove reasoning tokens from md file\n",
    "ChatHistory.remove_reasoning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Act 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.user_prompt = '''\n",
    "Mother gives Little Red Riding Hood a basket of food.\n",
    "Mother tells R to take the basket to her grandmother.\n",
    "R walks through the forest.\n",
    "'''\n",
    "\n",
    "Chat.write_scene(models['Deepseek R1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.user_prompt = '''\n",
    "A wolf approaches Little Red Riding Hood and asks where she is going.\n",
    "R tells the wolf she is visiting her grandmother.\n",
    "The W suggests she pick flowers for her grandmother.\n",
    "'''\n",
    "\n",
    "Chat.write_scene(models['Deepseek V3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.user_prompt = '''\n",
    "\n",
    "'''\n",
    "Chat.write_scene(models['Deepseek R1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
